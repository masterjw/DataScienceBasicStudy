{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML lab 03 - Linear Regression 의 cost 최소화\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplified hypothesis\n",
    "$$H(x) = Wx $$\n",
    "$$cost(W)=\\frac{1}{m} \\sum_{i=1}^m ( H( x^{(i)} ) - y^{(i)} )^2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/matplotlib/font_manager.py:280: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1383ed438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cost 함수의 시각화 - Gradient descent algorithm 적용가능한가?\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "W = tf.placeholder(tf.float32)\n",
    "# Our hypothesis for linear model X * W\n",
    "hypothesis = X * W\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# Variables for plotting cost function\n",
    "W_val = []\n",
    "cost_val = []\n",
    "for i in range(-30, 50):\n",
    "    feed_W = i * 0.1\n",
    "    curr_cost, curr_W = sess.run([cost, W], feed_dict={W: feed_W})\n",
    "    W_val.append(curr_W)\n",
    "    cost_val.append(curr_cost)\n",
    "\n",
    "# Show the cost function\n",
    "plt.plot(W_val, cost_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent\n",
    "\n",
    "$$cost(W)=\\frac{1}{m} \\sum_{i=1}^m ( W( x^{(i)} ) - y^{(i)} )^2$$\n",
    "\n",
    "$$W := W - \\alpha\\frac{1}{m} \\sum_{i=1}^m ( Wx^{(i)} - y^{(i)} )x^{(i)}$$\n",
    "\n",
    "\n",
    "```\n",
    "# Minimize: Gradient Descent using derivative: W -= learning_rate * derivative\n",
    "learning_rate = 0.1\n",
    "gradient = tf.reduce_mean((W * X - Y) * X)\n",
    "descent = W - learning_rate * gradient\n",
    "update = W.assign(descent)\n",
    "```\n",
    "\n",
    "```\n",
    "# Minimize: Gradient Descent Magic\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.88698 [ 0.56403315]\n",
      "1 0.252296 [ 0.76748437]\n",
      "2 0.0717643 [ 0.87599164]\n",
      "3 0.020413 [ 0.93386221]\n",
      "4 0.00580635 [ 0.96472651]\n",
      "5 0.00165158 [ 0.98118746]\n",
      "6 0.000469786 [ 0.98996663]\n",
      "7 0.000133629 [ 0.99464887]\n",
      "8 3.801e-05 [ 0.99714607]\n",
      "9 1.08112e-05 [ 0.99847794]\n",
      "10 3.07499e-06 [ 0.99918824]\n",
      "11 8.7482e-07 [ 0.99956703]\n",
      "12 2.48876e-07 [ 0.99976909]\n",
      "13 7.07958e-08 [ 0.99987686]\n",
      "14 2.01183e-08 [ 0.99993432]\n",
      "15 5.7322e-09 [ 0.99996495]\n",
      "16 1.63912e-09 [ 0.99998128]\n",
      "17 4.61196e-10 [ 0.99999005]\n",
      "18 1.31958e-10 [ 0.9999947]\n",
      "19 3.6291e-11 [ 0.9999972]\n",
      "20 1.05409e-11 [ 0.99999851]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Our hypothesis for linear model X * W\n",
    "hypothesis = X * W\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize: Gradient Descent using derivative: W -= learning_rate * derivative\n",
    "learning_rate = 0.1\n",
    "gradient = tf.reduce_mean((W * X - Y) * X)\n",
    "descent = W - learning_rate * gradient\n",
    "update = W.assign(descent)\n",
    "\n",
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(21):\n",
    "    sess.run(update, feed_dict={X: x_data, Y: y_data})\n",
    "    print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.21514 [ 0.31103426]\n",
      "1 1.82095 [ 0.37533772]\n",
      "2 1.4969 [ 0.43363953]\n",
      "3 1.23052 [ 0.48649985]\n",
      "4 1.01154 [ 0.53442651]\n",
      "5 0.831531 [ 0.57788002]\n",
      "6 0.683556 [ 0.61727786]\n",
      "7 0.561913 [ 0.65299857]\n",
      "8 0.461918 [ 0.68538535]\n",
      "9 0.379717 [ 0.7147494]\n",
      "10 0.312144 [ 0.74137276]\n",
      "11 0.256596 [ 0.76551133]\n",
      "12 0.210934 [ 0.78739697]\n",
      "13 0.173397 [ 0.80723989]\n",
      "14 0.14254 [ 0.82523084]\n",
      "15 0.117174 [ 0.8415426]\n",
      "16 0.0963224 [ 0.85633194]\n",
      "17 0.0791813 [ 0.86974096]\n",
      "18 0.0650905 [ 0.88189846]\n",
      "19 0.0535073 [ 0.89292127]\n",
      "20 0.0439854 [ 0.9029153]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Our hypothesis for linear model X * W\n",
    "hypothesis = X * W\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize: Gradient Descent Magic\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(100):\n",
    "    _, c, w = sess.run([train, cost, W], feed_dict={X: x_data, Y: y_data})\n",
    "    print(step, c, w)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional : compute_gradient and apply_gradient\n",
    "gradient에 추가적인 약을 치고 싶을 때 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.47548 [ 0.02069848]\n",
      "1 3.67904 [ 0.11209996]\n",
      "2 3.02434 [ 0.19497064]\n",
      "3 2.48614 [ 0.2701067]\n",
      "4 2.04372 [ 0.33823007]\n",
      "5 1.68003 [ 0.39999527]\n",
      "6 1.38106 [ 0.45599571]\n",
      "7 1.13529 [ 0.50676942]\n",
      "8 0.933259 [ 0.55280429]\n",
      "9 0.76718 [ 0.59454256]\n",
      "10 0.630656 [ 0.63238525]\n",
      "11 0.518427 [ 0.66669595]\n",
      "12 0.42617 [ 0.69780433]\n",
      "13 0.350331 [ 0.72600925]\n",
      "14 0.287988 [ 0.75158173]\n",
      "15 0.236739 [ 0.77476746]\n",
      "16 0.19461 [ 0.79578918]\n",
      "17 0.159978 [ 0.81484884]\n",
      "18 0.131509 [ 0.8321296]\n",
      "19 0.108106 [ 0.84779751]\n",
      "20 0.088868 [ 0.86200309]\n",
      "21 0.0730534 [ 0.87488282]\n",
      "22 0.0600532 [ 0.88656044]\n",
      "23 0.0493664 [ 0.89714813]\n",
      "24 0.0405813 [ 0.90674764]\n",
      "25 0.0333597 [ 0.91545117]\n",
      "26 0.0274231 [ 0.92334241]\n",
      "27 0.0225431 [ 0.93049711]\n",
      "28 0.0185314 [ 0.93698406]\n",
      "29 0.0152336 [ 0.94286555]\n",
      "30 0.0125227 [ 0.94819808]\n",
      "31 0.0102942 [ 0.95303291]\n",
      "32 0.00846231 [ 0.95741653]\n",
      "33 0.0069564 [ 0.96139097]\n",
      "34 0.00571846 [ 0.96499449]\n",
      "35 0.00470084 [ 0.96826166]\n",
      "36 0.0038643 [ 0.97122389]\n",
      "37 0.00317662 [ 0.97390968]\n",
      "38 0.00261133 [ 0.97634476]\n",
      "39 0.00214663 [ 0.97855258]\n",
      "40 0.00176462 [ 0.98055434]\n",
      "41 0.0014506 [ 0.98236924]\n",
      "42 0.00119245 [ 0.98401481]\n",
      "43 0.00098025 [ 0.98550677]\n",
      "44 0.000805808 [ 0.9868595]\n",
      "45 0.000662412 [ 0.98808593]\n",
      "46 0.000544532 [ 0.98919791]\n",
      "47 0.000447629 [ 0.99020612]\n",
      "48 0.000367971 [ 0.99112022]\n",
      "49 0.000302486 [ 0.99194902]\n",
      "50 0.000248657 [ 0.99270046]\n",
      "51 0.000204406 [ 0.99338174]\n",
      "52 0.000168033 [ 0.99399942]\n",
      "53 0.000138131 [ 0.99455947]\n",
      "54 0.000113549 [ 0.99506724]\n",
      "55 9.33444e-05 [ 0.99552763]\n",
      "56 7.67322e-05 [ 0.99594504]\n",
      "57 6.30773e-05 [ 0.99632353]\n",
      "58 5.18518e-05 [ 0.99666667]\n",
      "59 4.26237e-05 [ 0.99697781]\n",
      "60 3.50392e-05 [ 0.99725986]\n",
      "61 2.88037e-05 [ 0.99751562]\n",
      "62 2.36777e-05 [ 0.99774748]\n",
      "63 1.94645e-05 [ 0.99795771]\n",
      "64 1.60011e-05 [ 0.99814832]\n",
      "65 1.31524e-05 [ 0.99832118]\n",
      "66 1.08122e-05 [ 0.99847788]\n",
      "67 8.88864e-06 [ 0.99861991]\n",
      "68 7.30676e-06 [ 0.99874872]\n",
      "69 6.00685e-06 [ 0.99886549]\n",
      "70 4.93751e-06 [ 0.9989714]\n",
      "71 4.05835e-06 [ 0.99906743]\n",
      "72 3.33667e-06 [ 0.99915445]\n",
      "73 2.74255e-06 [ 0.99923337]\n",
      "74 2.25466e-06 [ 0.99930489]\n",
      "75 1.85358e-06 [ 0.99936974]\n",
      "76 1.52375e-06 [ 0.99942857]\n",
      "77 1.25258e-06 [ 0.99948192]\n",
      "78 1.0298e-06 [ 0.99953026]\n",
      "79 8.46443e-07 [ 0.99957412]\n",
      "80 6.95651e-07 [ 0.99961388]\n",
      "81 5.71898e-07 [ 0.99964994]\n",
      "82 4.70155e-07 [ 0.99968261]\n",
      "83 3.86457e-07 [ 0.99971223]\n",
      "84 3.1766e-07 [ 0.99973911]\n",
      "85 2.61042e-07 [ 0.99976349]\n",
      "86 2.14578e-07 [ 0.99978554]\n",
      "87 1.76368e-07 [ 0.99980557]\n",
      "88 1.45023e-07 [ 0.99982369]\n",
      "89 1.19295e-07 [ 0.99984014]\n",
      "90 9.80605e-08 [ 0.99985504]\n",
      "91 8.06248e-08 [ 0.99986857]\n",
      "92 6.62368e-08 [ 0.99988085]\n",
      "93 5.44356e-08 [ 0.999892]\n",
      "94 4.47434e-08 [ 0.99990207]\n",
      "95 3.6829e-08 [ 0.99991119]\n",
      "96 3.0251e-08 [ 0.99991947]\n",
      "97 2.48881e-08 [ 0.99992698]\n",
      "98 2.04563e-08 [ 0.99993378]\n",
      "99 1.68051e-08 [ 0.99993998]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Our hypothesis for linear model X * W\n",
    "hypothesis = X * W\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize: Gradient Descent Magic\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "# Get gradients\n",
    "gvs = optimizer.compute_gradients(cost)\n",
    "\n",
    "### Additional compute gradients\n",
    "\n",
    "# apply gradients\n",
    "apply_gradients = optimizer.apply_gradients(gvs)\n",
    "\n",
    "\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(100):\n",
    "    _, c, w = sess.run([train, cost, W], feed_dict={X: x_data, Y: y_data})\n",
    "    print(step, c, w)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
